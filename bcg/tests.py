# AUTOGENERATED! DO NOT EDIT! File to edit: notebooks/00_tests.ipynb (unless otherwise specified).

__all__ = ['stochastically_convert_to_binary', 'create_nxgraph', 'create_linear_dataset', 'get_feature_importance',
           'partial_dependence', 'plot_partial_dependency']

# Cell
import numpy as np
import pandas as pd
import matplotlib as mpl
import matplotlib.pyplot as plt

import dowhy
from dowhy import CausalModel
import dowhy.datasets
import networkx as nx
from sklearn import ensemble, metrics
import typing
import itertools

# Cell
def stochastically_convert_to_binary(x:float):
    p = 1/(1+np.exp(-x))  # sigmoid
    return np.random.choice([0, 1], size=1, p=[1-p, p])

# Cell
def create_nxgraph(treatments:typing.List[str], outcome:str, common_causes:typing.List[str],
                   instruments:typing.List[str], effect_modifiers:typing.List[str]=[],
                   unobs_conf:str="Unobserved Confounder"):
    'Created a specific directed acyclic graph used for the `create_linear_dataset` function.'
    g = nx.DiGraph()

    g.add_edge(unobs_conf, outcome)
    g.add_edges_from([(treatment, outcome) for treatment in treatments])
    g.add_edges_from([(unobs_conf, treatment) for treatment in treatments])
    g.add_edges_from([(common_cause, treatment)
                      for common_cause, treatment in itertools.product(common_causes, treatments)])
    g.add_edges_from([(instrument, treatment)
                      for instrument, treatment in itertools.product(instruments, treatments)])
    g.add_edges_from([(common_cause, outcome) for common_cause in common_causes])
    g.add_edges_from([(effect_modifier, outcome) for effect_modifier in effect_modifiers])
    return g

# Cell
def create_linear_dataset(beta:typing.Union[int,float,list,np.ndarray],
                   num_treatments:int, num_common_causes:int, num_samples:int,
                   num_instruments:int, num_effect_modifiers:int,
                   treatment_is_binary:bool,
                   outcome_is_binary:bool):
    '''Generates data using a linear model

    treament: $t = \varepsilon + W \cdot c_1 + Z \cdot c_z$
    output: $y = t \cdot \beta + W \cdot c_2 + X \cdot c_e \cdot t$

    Names assigned in ret_dict['df']
    * `V`: treatment
    * `Y`: outcome
    * `W`: common cause
    * `Z`: instrument
    * `x`: effect modifier
    '''
    W, X, Z, c1, c2, ce, cz = [None]*7

    beta = float(beta)

    if not isinstance(beta, (list, np.ndarray)):
        beta = np.repeat(beta, num_treatments)

    if num_common_causes > 0:
        range_c1 = max(beta) * .5
        range_c2 = max(beta) * .5
        means = np.random.uniform(-1, 1, num_common_causes)
        cov_mat = np.diag(np.ones(num_common_causes))
        W = np.random.multivariate_normal(means, cov_mat, size=num_samples)
        c1 = np.random.uniform(0, range_c1, size=(num_common_causes, num_treatments))
        c2 = np.random.uniform(0, range_c2, size=num_common_causes)

    if num_instruments > 0:
        range_cz = beta
        p = np.random.uniform(0, 1, num_instruments)
        Z = np.zeros((num_samples, num_instruments))
        for i in range(num_instruments):
            if (i % 2) == 0:
                Z[:, i] = np.random.binomial(n=1, p=p[i], size=num_samples)  # ???
            else:
                Z[:, i] = np.random.uniform(0, 1, size=num_samples)  # ???
        # TODO Ensure that we do not generate weak instruments
        cz = np.random.uniform(low=range_cz - range_cz * .05,
                               high=range_cz + range_cz * .05,
                               size=(num_instruments, num_treatments))

    if num_effect_modifiers >0:
        range_ce = beta * .5
        means = np.random.uniform(-1, 1, num_effect_modifiers)
        cov_mat = np.diag(np.ones(num_effect_modifiers))
        X = np.random.multivariate_normal(means, cov_mat, num_samples)
        ce = np.random.uniform(0, range_ce, num_effect_modifiers)
    # TODO - test all our methods with random noise added to covariates (instead of the stochastic treatment assignment)

    t = np.random.normal(0, 1, size=(num_samples, num_treatments))

    if num_common_causes > 0:
        t += W @ c1

    if num_instruments > 0:
        t += Z @ cz

    # Converting treatment to binary if required
    if treatment_is_binary:
        t = np.vectorize(stochastically_convert_to_binary)(t)


    def _compute_y(t, W, X, beta, c2, ce):
        y =  t @ beta
        if num_common_causes > 0:
            y += W @ c2
        if num_effect_modifiers > 0:
            y += (X @ ce) * np.prod(t, axis=1)
        return y

    y = _compute_y(t, W, X, beta, c2, ce)

    if outcome_is_binary:
        y = np.vectorize(stochastically_convert_to_binary)(t)


    data = np.column_stack((t, y))
    if num_common_causes > 0:
        data = np.column_stack((W, data))
    if num_instruments > 0:
        data = np.column_stack((Z, data))
    if num_effect_modifiers > 0:
        data = np.column_stack((X, data))

    effect_modifiers = [f'X{i}' for i in range(num_effect_modifiers)]
    instruments = [f'Z{i}' for i in range(num_instruments)]
    common_causes = [f'W{i}' for i in range(num_common_causes)]
    treatments = [f'v{i}' for i in range(num_treatments)]
    outcome = ['y']
    data = pd.DataFrame(
        data=data,
        columns=effect_modifiers + instruments + common_causes \
                + treatments + outcome
    )

    # Specifying the correct dtypes
    if treatment_is_binary:
        data = data.astype({tname:'bool' for tname in treatments}, copy=False)
    if outcome_is_binary:
        data = data.astype({outcome: 'bool'}, copy=False)

    ate = np.mean(_compute_y(np.ones((num_samples, num_treatments)), W, X, beta, c2, ce)
                  - _compute_y(np.zeros((num_samples, num_treatments)), W, X, beta, c2, ce))

    # Now writing the gml graph
    g = create_nxgraph(treatments, outcome[0], common_causes, instruments, effect_modifiers)
    gml_graph = ''.join([v for v in nx.readwrite.gml.generate_gml(g)])
    ret_dict = {
        "df": data,
        "treatment_name": treatments,
        "outcome_name": outcome[0],
        "common_causes_names": common_causes,
        "instrument_names": instruments,
        "effect_modifier_names": effect_modifiers,
        "gml_graph": gml_graph,
        "g": g,
        "average_treatment_effect": ate
    }
    return ret_dict

# Cell
def get_feature_importance(m, X:np.ndarray, y:np.ndarray, x_cols:list):
    n_obs, n_row = X.shape
    scores = {}
    for i in range(n_row):
        _X = X.copy()
        np.random.shuffle(_X[:,i])
        _y = m.predict(_X)
        scores[i] = metrics.mean_squared_error(y, _y)

    scores = pd.DataFrame([{'variable': x_cols[i], 'feature_importance': scores[i]} for i in scores])
    scores.sort_values('feature_importance', ascending=False, inplace=True)
    return scores

# Cell
def partial_dependence(m, X:np.ndarray, y:np.ndarray, x_cols:list,
                       idp_col:str, max_num_obs:int=100, max_num_ys:int=10):
    n_obs, n_row = X.shape
    ys = {}
    ix = x_cols.index(idp_col)

    idp_vals = np.unique(X[:,ix])
    idp_vals = idp_vals[::len(idp_vals)//max_num_ys]

    if max_obs:
        if max_obs >= n_obs:
            ixs = np.arange(n_obs)
        else:
            ixs = np.random.choice(np.arange(n_obs), size=max_num_obs,
                                   replace=False)
    for i, val in enumerate(idp_vals):
        _X = X[ixs,:].copy()
        _X[:,ix] = val
        _y = m.predict(_X)
        ys[i] = _y.copy()

    part_deps = pd.DataFrame({val: ys[i] for i,val in enumerate(idp_vals)})
    return part_deps

# Cell
def plot_partial_dependency(part_dep_res:pd.DataFrame, idp_col:str,
                            target:str):
    fig, ax = plt.subplots(figsize=(8,4), constrained_layout=True)
    x = part_dep_res.columns.values
    for i, row in part_dep_res.iterrows():
        ax.plot(x, row.values, alpha=.1, lw=1, color='black')
    ax.set(xlabel=f'"{idp_col}"', ylabel=f'"{target}"', title=f'Partial dependency plot "{target}" vs "{idp_col}"')
    plt.show()